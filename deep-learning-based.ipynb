{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install datasets\n# !pip install accelerate -U","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset\ndataset = load_dataset('GaryHuang/NTU_geolocation')\nfull_dataset = dataset['train']\n\n# Convert string labels to numeric labels\nlabel_encoder = LabelEncoder()\nnumerical_labels = label_encoder.fit_transform(full_dataset['label'])  # Replace 'label' with the actual label column name if different\n\n# Replace the original labels with numeric labels\nfull_dataset = full_dataset.remove_columns('label')\nfull_dataset = full_dataset.add_column('label', numerical_labels)\n\n# Define the sample size (e.g., 10% of the full dataset)\nsample_size = int(1 * len(full_dataset))  # Adjust this percentage based on your needs\n\n# Shuffle the dataset and sample a subset\nnp.random.seed(42)  # For reproducibility\nsample_indices = np.random.permutation(len(full_dataset))[:sample_size]\nsampled_dataset = full_dataset.select(sample_indices)\n\n# Define the sizes for train, validation, and test split: 80%, 10%, 10%\ntrain_size = int(0.8 * len(sampled_dataset))\nvalidation_size = int(0.1 * len(sampled_dataset))\ntest_size = len(sampled_dataset) - train_size - validation_size  # Ensure all samples are used\n\n# Generate shuffled indices for the sampled dataset\nindices = np.random.permutation(len(sampled_dataset))\n\n# Split indices for each dataset\ntrain_indices = indices[:train_size]\nvalidation_indices = indices[train_size:train_size + validation_size]\ntest_indices = indices[train_size + validation_size:]\n\n# Create subsets using the indices\ntrain_subset = sampled_dataset.select(train_indices)\nvalidation_subset = sampled_dataset.select(validation_indices)\ntest_subset = sampled_dataset.select(test_indices)\n\n# Print the sizes to confirm\nprint(f\"Train subset size: {len(train_subset)}\")\nprint(f\"Validation subset size: {len(validation_subset)}\")\nprint(f\"Test subset size: {len(test_subset)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandAugment(num_ops = 5, magnitude = 10),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataset, transform=None):\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        image = Image.fromarray(np.array(item['image']))\n        label = item['label']\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\ndef create_dataloaders(train_subset, validation_subset, test_subset, batch_size=32):\n    train_dataset = CustomDataset(train_subset, transform=data_transforms['train'])\n    val_dataset = CustomDataset(validation_subset, transform=data_transforms['val'])\n    test_dataset = CustomDataset(test_subset, transform=data_transforms['test'])\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    \n    return train_loader, val_loader, test_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_loader, val_loader, test_loader = create_dataloaders(train_subset, validation_subset, test_subset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter out any None values that might be returned due to image loading errors\ntrain_dataset = [(image, label) for image, label in train_subset if image is not None]\nvalidation_dataset = [(image, label) for image, label in validation_subset if image is not None]\ntest_dataset = [(image, label) for image, label in test_subset if image is not None]\n\n# Create data loaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalidation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nfrom tqdm import tqdm\nimport timm\n\nmodel = timm.create_model('resnet18', pretrained=False)\nprint(model)\nmodel.fc = nn.Linear(model.fc.in_features, len(label_encoder.classes_))\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\ndef train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=1):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for images, labels in tqdm(train_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * images.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n\n        model.eval()\n        validation_loss = 0.0\n        corrects = 0\n        with torch.no_grad():\n            for images, labels in validation_loader:\n                images = images.to(device)\n                labels = labels.to(device)\n\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                validation_loss += loss.item() * images.size(0)\n\n                _, preds = torch.max(outputs, 1)\n                corrects += torch.sum(preds == labels.data)\n\n        validation_loss = validation_loss / len(validation_loader.dataset)\n        accuracy = corrects.double() / len(validation_loader.dataset)\n        print(f\"Validation Loss: {validation_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\ntrain_loader, val_loader, test_loader = create_dataloaders(train_subset, validation_subset, test_subset)\n\ntrain_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model.pt')\nprint(\"Model saved to model.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\ndef test_model(model, test_loader):\n    model.eval()\n    corrects = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(test_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            \n            corrects += torch.sum(preds == labels.data)\n            total += labels.size(0)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = corrects.double() / total\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n\n    return all_preds, all_labels\n\nall_preds, all_labels = test_model(model, test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, f1_score, roc_curve, auc, roc_auc_score, accuracy_score\nimport seaborn as sns\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\n# Assuming all_labels and all_preds are numpy arrays\nall_labels = np.array(all_labels)\nall_preds = np.array(all_preds)\n\n# Ensure the model is in evaluation mode\nmodel.eval()\n\n# Get the probability scores for the positive class\nall_probs = []\nwith torch.no_grad():\n    for images, _ in tqdm(test_loader):\n        images = images.to(device)\n        outputs = model(images)\n        probs = torch.softmax(outputs, dim=1)\n        all_probs.extend(probs.cpu().numpy())\n\nall_probs = np.array(all_probs)\n\n# Calculate evaluation metrics\ncm = confusion_matrix(all_labels, all_preds)\nf1 = f1_score(all_labels, all_preds, average='weighted')\naccuracy = accuracy_score(all_labels, all_preds)\nroc_auc_macro = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')\nroc_auc_weighted = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n\n# Plot and save confusion matrix\nplt.figure(figsize=(10, 7))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.savefig('confusion_matrix.png')\nplt.close()\n\n# Calculate ROC curve and AUC for each class\nn_classes = len(label_encoder.classes_)\nfpr = {}\ntpr = {}\nroc_auc = {}\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(all_labels == i, all_probs[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Plot and save ROC curve\nplt.figure()\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.savefig('roc_curve.png')\nplt.close()\n\n# Write report to text file\nwith open('evaluation_report.txt', 'w') as f:\n    f.write(f\"Evaluation Accuracy: {accuracy:.4f}\\n\")\n    f.write(f\"F1 Score: {f1:.4f}\\n\")\n    f.write(f\"Macro AUC: {roc_auc_macro:.4f}\\n\")\n    f.write(f\"Weighted AUC: {roc_auc_weighted:.4f}\\n\")\n    f.write(\"\\nClass-wise AUC Scores:\\n\")\n    for i in range(n_classes):\n        f.write(f\"Class {i} AUC: {roc_auc[i]:.4f}\\n\")\n\nprint(\"Evaluation report saved to evaluation_report.txt\")\nprint(\"Confusion matrix image saved to confusion_matrix.png\")\nprint(\"ROC curve image saved to roc_curve.png\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}